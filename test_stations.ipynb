{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of working with GHCN data from Romania\n",
    "\n",
    "[Global historcal climate network](http://www.ncdc.noaa.gov/ghcnm/) weather station data from Romania.\n",
    "\n",
    "N.B. Parts of this work will only function with Pandas version > 0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load some libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: data preperation\n",
    "\n",
    "From our Station Data files we need to create:\n",
    "* one single data structure\n",
    "* Date indexed data (with only one index for all the datasets)\n",
    "* Station names as column identifiers\n",
    "\n",
    "We will use the [Pandas library](http://pandas.pydata.org/pandas-docs/stable/index.html), as it is perfectly suited to our task. We have read it in above with the alias **pd**.\n",
    "\n",
    "Before we try and read all the data, let's test a procedure with one single station file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read a station data file with Pandas\n",
    "test_data = pd.read_csv(\"Data/station_data/BUM00015502_VIDIN_BU_.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, but the Dates should be an index, not a column, and they should also be a date object, not a simple integer (we get much more functionality that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list of datetime values out of the integer dates using a list comprehension technique\n",
    "dates = []\n",
    "for date in test_data['DATE']:\n",
    "    dates.append(pd.datetime.strptime(str(date),\"%Y%m%d\"))\n",
    "# The above could have been done more effectivley using list comprehension\n",
    "\n",
    "# Next set the new list as an index, and remove the old column from the dataset\n",
    "test_data.index = dates\n",
    "test_data = test_data.drop(['DATE','PRCP'], axis=1)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can plot a simple preview of the data to make sure it looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preview plot is messy as our data are not contiguous, but as a quick-check, it seems like everything is more-or-less fine.\n",
    "\n",
    "So, reading a single file is easy, and straightforward. But we want to do some exploratory analysis on multiple station measurements. For this we will need to read all the station data together into a consistent data object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a small tools (functions) to help with the work\n",
    "\n",
    "def station_name(fname):\n",
    "    \"\"\"Return the station ID from a path/filename.csv string\"\"\"\n",
    "    tmp = fname.split('/')[-1]\n",
    "    return tmp.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we were on a Mac or Linux system, we could get the file list via a bash command\n",
    "flist = !ls Data/station_data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But this will break on windows. To make our code cross-platform we use a python\n",
    "# library to find all the files instead. This is much better than hard-coding the files!\n",
    "\n",
    "frames = [] # an empty list to hold each data object as it is loaded\n",
    "\n",
    "mypath = 'Data/station_data/'          # Set path to data\n",
    "for item in tqdm(os.listdir(mypath)):        # Find all files in that path and loop over them\n",
    "    if '.csv' in item:                 # If the file is a csv type do something...\n",
    "        fname = ''.join([mypath,item])\n",
    "        station = station_name(fname)\n",
    "        #print('\\rReading data from station', station, end='')\n",
    "        tmp = pd.read_csv(fname)\n",
    "        dates = [pd.datetime.strptime(str(date),\"%Y%m%d\") for date in tmp['DATE']]\n",
    "        tmp.index = dates\n",
    "        tmp = tmp.drop(['DATE','PRCP'], axis=1) # get rid of date and precipitation columns\n",
    "        tmp.columns = [station]     # Re-name TAVG to be the station name\n",
    "        frames.append(tmp)\n",
    "#print('\\rDone reading data.')\n",
    "print(\"{0} GHCN files read\".format(len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(frames)  # Join all the seperate data together into one object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: Cleaning the dataset for analysis\n",
    "\n",
    "Now we have created a dataframe **df** holding all the station data with one coherant time index.\n",
    "\n",
    "This abstraction will do much of the work for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First lets see how long these data run for in time\n",
    "print(\"minimum date:\", min(df.index).date())\n",
    "print(\"maximum date:\", max(df.index).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's look at a statistical description of these data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a clear problem with these stats. Most of these data seem to have a missing value of `-9999.0` included.\n",
    "To proceede we should replace with with a missing data type that we can operate with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can replace all -999.0 values with np.nan like this\n",
    "\n",
    "df[df == -9999.0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, the dataframe values seems reasonable, except we can see there are many series which are empty.\n",
    "# They were just full of missing values for whatever reason.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It looks like we can simply filter out data that now has a low count (e.g. < 10,000).\n",
    "\n",
    "limit = 10000\n",
    "\n",
    "for key in df:\n",
    "    if df[key].count() <= limit:\n",
    "        print('removing', key,'from df object.')\n",
    "        df = df.drop([key], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Much better! Finally a clean df object, that we can work from.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Analysis\n",
    "Individually, the station data is still patchy and potentially poor. It also looks like there are still some bad values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in df:\n",
    "    plt.plot(df[key], lw=1., alpha=0.5)\n",
    "plt.title('Individual stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated though, it gives a much clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1).plot(title='Romanian TAVG', lw=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the average data a new series, and strip out nan values for working ease\n",
    "\n",
    "df_mean = pd.DataFrame(df.mean(axis=1), columns=['mean_temp'])  # make a new df object\n",
    "df_mean = df_mean[df_mean.notnull().values]                     # remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_mean.rolling(300, center=True).mean().plot(color='k', lw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_mean\n",
    "\n",
    "\n",
    "# Day of year mean...\n",
    "\n",
    "# Deseasnalise data with DOY mean\n",
    "\n",
    "# Examine average temperature anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Using the data for something useful!\n",
    "\n",
    "Based on historical Romanian average temperature anomalies, how does a given value rank?\n",
    "\n",
    "Requires an average temperature and a date as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.mean(axis=1).plot.hist(bins=150, normed=True, xlim=[-30,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract a ranked list of the valid values\n",
    "\n",
    "tmp = df.mean(axis=1, skipna=True)\n",
    "tmp = tmp[tmp.notnull()]\n",
    "ranked = np.array(sorted(tmp.values))\n",
    "\n",
    "plt.plot(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b,c = plt.hist(ranked, bins=150, cumulative=True, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Needs to be anomalies\n",
    "x = 30\n",
    "pop = len(ranked)\n",
    "print(\"{0} values in population\".format(pop))\n",
    "mask = x < ranked\n",
    "larger = len(ranked[mask])\n",
    "print(\"{0} values are greater than {1}C\".format(larger, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
