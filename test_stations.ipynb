{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of working with GHCN data from Romania\n",
    "\n",
    "[Global historcal climate network](http://www.ncdc.noaa.gov/ghcnm/) weather station data from Romania.\n",
    "\n",
    "Requires Pandas version > 0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load some libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import *\n",
    "from colorama import Fore\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: data preperation\n",
    "\n",
    "From our Station Data files we need to create:\n",
    "* one single data structure\n",
    "* Date indexed data (with only one index for all the datasets)\n",
    "* Station names as column identifiers\n",
    "\n",
    "We will use the [Pandas library](http://pandas.pydata.org/pandas-docs/stable/index.html), as it is perfectly suited to our task. We have read it in above with the alias **pd**.\n",
    "\n",
    "Before we try and read all the data, let's test a procedure with one single station file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read a station data file with Pandas\n",
    "test_data = pd.read_csv(\"Data/station_data/BUM00015502_VIDIN_BU_.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, but the Dates should be an index, not a column, and they should also be a date object, not a simple integer (we get much more functionality that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a list of datetime values out of the integer dates using a list comprehension technique\n",
    "dates = []\n",
    "for date in test_data['DATE']:\n",
    "    dates.append(pd.datetime.strptime(str(date),\"%Y%m%d\"))\n",
    "# The above could have been done more effectivley using list comprehension\n",
    "\n",
    "# Next set the new list as an index, and remove the old column from the dataset\n",
    "test_data.index = dates\n",
    "test_data = test_data.drop(['DATE','PRCP'], axis=1)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can plot a simple preview of the data to make sure it looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preview plot is messy as our data are not contiguous, but as a quick-check, it seems like everything is more-or-less fine.\n",
    "\n",
    "So, reading a single file is easy, and straightforward. But we want to do some exploratory analysis on multiple station measurements. For this we will need to read all the station data together into a consistent data object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a small tools (functions) to help with the work\n",
    "\n",
    "def station_name(fname):\n",
    "    \"\"\"Return the station ID from a path/filename.csv string\"\"\"\n",
    "    tmp = fname.split('/')[-1]\n",
    "    return tmp.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we were on a Mac or Linux system, we could get the file list via a bash command\n",
    "flist = !ls Data/station_data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But this will break on windows. To make our code cross-platform we use a python\n",
    "# library to find all the files instead. This is much better than hard-coding the files!\n",
    "\n",
    "frames = [] # an empty list to hold each data object as it is loaded\n",
    "\n",
    "mypath = 'Data/station_data/'          # Set path to data\n",
    "for item in tqdm(os.listdir(mypath)):        # Find all files in that path and loop over them\n",
    "    if '.csv' in item:                 # If the file is a csv type do something...\n",
    "        fname = ''.join([mypath,item])\n",
    "        station = station_name(fname)\n",
    "        #print('Reading data from station', station)\n",
    "        tmp = pd.read_csv(fname)\n",
    "        dates = [pd.datetime.strptime(str(date),\"%Y%m%d\") for date in tmp['DATE']]\n",
    "        tmp.index = dates\n",
    "        tmp = tmp.drop(['DATE','PRCP'], axis=1) # get rid of date and precipitation columns\n",
    "        tmp.columns = [station]     # Re-name TAVG to be the station name\n",
    "        frames.append(tmp)\n",
    "print(\"{0} GHCN files read\".format(len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(frames, axis=1)  # Join all the seperate data together into one object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: Cleaning the dataset for analysis\n",
    "\n",
    "Now we have created a dataframe **df** holding all the station data with one coherant time index.\n",
    "\n",
    "This abstraction will do much of the work for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First lets see how long these data run for in time\n",
    "print(\"minimum date:\", min(df.index).date())\n",
    "print(\"maximum date:\", max(df.index).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's look at a statistical description of these data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a clear problem with these stats. Most of these data seem to have a missing value of `-9999.0` included.\n",
    "To proceede we should replace with with a missing data type that we can operate with `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can replace all -999.0 values with np.nan like this\n",
    "df[df == -9999.0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, the dataframe values seems reasonable, except we can see there are many series which are empty.\n",
    "# They were just full of missing values for whatever reason.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It looks like we can simply filter out data that now has a low count (e.g. < 10,000).\n",
    "\n",
    "limit = 0\n",
    "\n",
    "for key in df:\n",
    "    if df[key].count() <= limit:\n",
    "        print('removing', key,'from df object.')\n",
    "        df = df.drop([key], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Much better! Finally a clean df object, that we can work from.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Creating a mean time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in df:\n",
    "    plt.plot(df[key], lw=0.3, alpha=0.75)\n",
    "plt.title('Individual stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated, these data gives a clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1).plot(title='Romanian TAVG', lw=0.5, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data it is easy to see the seasonal signal is dominant. One method of removing the seasonal signal and work with anomalies is to subtract a day-of-the-year (DOY) mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the average data a new series, and strip out nan values for working ease\n",
    "df_mean = pd.DataFrame(df.mean(axis=1), columns=['mean_temp'])  # make a new df object\n",
    "#df_mean = df_mean[df_mean.notnull().values]                     # remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can look at every day in the time series, find its DOY value (an integer from 1 - 366), make a corresponding list, and use it to subscript the mean dataframe, and find the mean on each day of the year. I place these values in a dictionary with integer doy as a key, for ease of use later when deseasonalising.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doy_index = np.array([ date.dayofyear for date in df_mean.index])  # make an array of DOY values to use as a lookup\n",
    "d = {}\n",
    "for doy in range(1,367):\n",
    "    d[doy] = df_mean[doy_index == doy].mean().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Veryify it looks okay...\n",
    "tmp = []\n",
    "for key in d:\n",
    "    tmp.append(d[key])\n",
    "plt.plot(range(1,367), tmp)\n",
    "plt.title(\"Seasonal Temperature signal in Romania\")\n",
    "plt.xlabel('DOY')\n",
    "plt.xlim([1,367])\n",
    "plt.ylabel(\"Temp. Â°C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Remove the seasonal signal from these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dszn = []\n",
    "for day in df_mean.index:\n",
    "    dszn.append(df_mean['mean_temp'][day] - d[day.dayofyear])\n",
    "dszn = np.array(dszn)\n",
    "\n",
    "df_mean['anom'] = dszn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All of the above could have just been done in one line...\n",
    "# df_mean['anom'] = np.array([df_mean['mean_temp'][day] - d[day.dayofyear] for day in df_mean.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(df_mean.index, df_mean.anom, lw=0.5, alpha = 0.75)\n",
    "plt.title(r\"Romanina $\\delta$ Temp. Â°C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using the data for something useful!\n",
    "\n",
    "Based on historical Romanian average temperature anomalies, how does a given value rank?\n",
    "\n",
    "Requires an average temperature and a date as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anom_of_given_date(date, temperature):\n",
    "    \"\"\"Given a specific date and temperature, return the delta temp value\"\"\"\n",
    "    qtmp =pd.DataFrame([temperature], index=[pd.datetime.strptime(str(date),\"%Y%m%d\")])\n",
    "    qanom = temperature - d[qtmp.index[0].dayofyear]\n",
    "    print(\"Day of year for {0} is {1}\".format(date, qtmp.index[0].dayofyear))\n",
    "    print(\"ð›¿ temp. = {0:3.2f}Â°C\".format(qanom))\n",
    "    return qanom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anom = anom_of_given_date(date=19830901, temperature=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, bins, c = plt.hist(df_mean.anom, bins=80, normed=True)\n",
    "plt.title(r\"Romanian $\\delta$ temperature\")\n",
    "plt.xlabel(r\"$\\delta$ Temp. Â°C\")\n",
    "plt.ylim(0,0.13)\n",
    "plt.vlines(anom, 0, 0.16, label=\"day to check\")\n",
    "\n",
    "y = matplotlib.mlab.normpdf(bins, df_mean.anom.mean(), df_mean.anom.std())\n",
    "plt.plot(bins, y, label='Gaussian pdf')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's see that in a cumulative density plot as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b,c = plt.hist(df_mean.anom, bins=100, cumulative=True, normed=True)\n",
    "plt.vlines(anom, 0, 1.1)\n",
    "plt.ylim(0,1)\n",
    "plt.title(r\"Cumulative $\\delta$ distribution and value to test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rank the anomaly values, and count how many times our value to check is larger than the ranked values. Divide this by the total number of values to get the percentile position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked = np.array(sorted(df_mean.anom.values))\n",
    "print(\"ð›¿ of {0:3.2f}Â°C has a percentile rank of {1:3.2f}\".format(anom, len(ranked[anom > ranked]) / len(ranked)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finally, combine the above into a function that can be re-run easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_anomaly(date, temperature):\n",
    "    anom = anom_of_given_date(date=date, temperature=temperature)\n",
    "    a,b,c = plt.hist(df_mean.anom, bins=100, cumulative=True, normed=True)\n",
    "    plt.vlines(anom, 0, 1.1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(r\"Cumulative $\\delta$ distribution\")\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel(r\"$\\delta$ T. [Â°C]\")\n",
    "    plt.show()\n",
    "    ranked = np.array(sorted(df_mean.anom.values))\n",
    "    print(\"ð›¿ of {0:3.2f}Â°C has a \".format(anom),end=\"\")\n",
    "    print(Fore.RED + \"percentile rank of {0:3.2f}\".format(len(ranked[anom > ranked]) / len(ranked)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_anomaly(date=20161020, temperature=15)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
